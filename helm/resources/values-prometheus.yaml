## prometheus configuration
## Ref: https://github.com/kubernetes/charts/blob/master/stable/prometheus/values.yaml
alertmanagerFiles:
  alertmanager.yml:
    global:
      slack_api_url: '${extmd.slackUrl}'
    receivers:
    - name: slack
      slack_configs:
      - channel: '${extmd.slackChannel}'
        send_resolved: true
    route:
      group_wait: 10s
      group_interval: 5m
      receiver: slack
      repeat_interval: 3h
    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname']
serverFiles:
  rules:
    groups:
    - name: extendedmind.rules
      rules:
      - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
        expr: histogram_quantile(0.99, sum by(le, cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.99"
      - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
        expr: histogram_quantile(0.9, sum by(le, cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.9"
      - record: cluster:scheduler_e2e_scheduling_latency:quantile_seconds
        expr: histogram_quantile(0.5, sum by(le, cluster) (scheduler_e2e_scheduling_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.5"
      - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
        expr: histogram_quantile(0.99, sum by(le, cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.99"
      - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
        expr: histogram_quantile(0.9, sum by(le, cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.9"
      - record: cluster:scheduler_scheduling_algorithm_latency:quantile_seconds
        expr: histogram_quantile(0.5, sum by(le, cluster) (scheduler_scheduling_algorithm_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.5"
      - record: cluster:scheduler_binding_latency:quantile_seconds
        expr: histogram_quantile(0.99, sum by(le, cluster) (scheduler_binding_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.99"
      - record: cluster:scheduler_binding_latency:quantile_seconds
        expr: histogram_quantile(0.9, sum by(le, cluster) (scheduler_binding_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.9"
      - record: cluster:scheduler_binding_latency:quantile_seconds
        expr: histogram_quantile(0.5, sum by(le, cluster) (scheduler_binding_latency_microseconds_bucket))
          / 1e+06
        labels:
          quantile: "0.5"
      - alert: K8SNodeDown
        expr: up{job="kubelet"} == 0
        for: 1h
        labels:
          service: k8s
          severity: warning
        annotations:
          description: Prometheus could not scrape a {{ $labels.job }} for more than one
            hour
          summary: Kubelet cannot be scraped
      - alert: K8SNodeNotReady
        expr: kube_node_status_ready{condition="true"} == 0
        for: 1h
        labels:
          service: k8s
          severity: warning
        annotations:
          description: The Kubelet on {{ $labels.node }} has not checked in with the API,
            or has set itself to NotReady, for more than an hour
          summary: Node status is NotReady
      - alert: K8SManyNodesNotReady
        expr: count by(cluster) (kube_node_status_ready{condition="true"} == 0) > 1 and
          (count by(cluster) (kube_node_status_ready{condition="true"} == 0) / count by(cluster)
          (kube_node_status_ready{condition="true"})) > 0.2
        for: 1m
        labels:
          service: k8s
          severity: critical
        annotations:
          description: '{{ $value }} K8s nodes (more than 10% of cluster {{ $labels.cluster
            }}) are in the NotReady state.'
          summary: Many K8s nodes are Not Ready
      - alert: K8SKubeletNodeExporterDown
        expr: up{job="node-exporter"} == 0
        for: 15m
        labels:
          service: k8s
          severity: warning
        annotations:
          description: Prometheus could not scrape a {{ $labels.job }} for more than one
            hour.
          summary: Kubelet node_exporter cannot be scraped
      - alert: K8SApiserverDown
        expr: up{job="kubernetes"} == 0
        for: 15m
        labels:
          service: k8s
          severity: warning
        annotations:
          description: An API server could not be scraped.
          summary: API server unreachable
      - alert: K8SApiserverDown
        expr: absent({job="kubernetes"}) or (count by(cluster) (up{job="kubernetes"} ==
          1) < count by(cluster) (up{job="kubernetes"}))
        for: 5m
        labels:
          service: k8s
          severity: critical
        annotations:
          description: Prometheus failed to scrape multiple API servers, or all API servers
            have disappeared from service discovery.
          summary: API server unreachable
      - alert: K8SConntrackTableFull
        expr: 100 * node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 50
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          description: The nf_conntrack table is {{ $value }}% full.
          summary: Number of tracked connections is near the limit
      - alert: K8SConntrackTableFull
        expr: 100 * node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 90
        labels:
          service: k8s
          severity: critical
        annotations:
          description: The nf_conntrack table is {{ $value }}% full.
          summary: Number of tracked connections is near the limit
      - alert: K8SConntrackTuningMissing
        expr: node_nf_conntrack_udp_timeout > 10
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          description: Nodes keep un-setting the correct tunings, investigate when it
            happens.
          summary: Node does not have the correct conntrack tunings
      - alert: K8STooManyOpenFiles
        expr: 100 * process_open_fds{job=~"kubelet|kubernetes"} / process_max_fds > 50
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          description: '{{ $labels.node }} is using {{ $value }}% of the available file/socket
            descriptors.'
          summary: '{{ $labels.job }} has too many open file descriptors'
      - alert: K8STooManyOpenFiles
        expr: 100 * process_open_fds{job=~"kubelet|kubernetes"} / process_max_fds > 80
        for: 10m
        labels:
          service: k8s
          severity: critical
        annotations:
          description: '{{ $labels.node }} is using {{ $value }}% of the available file/socket
            descriptors.'
          summary: '{{ $labels.job }} has too many open file descriptors'
      - alert: K8SApiServerLatency
        expr: histogram_quantile(0.99, sum without(instance, node, resource) (apiserver_request_latencies_bucket{verb!~"CONNECT|WATCHLIST|WATCH"}))
          / 1e+06 > 1
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          description: 99th percentile Latency for {{ $labels.verb }} requests to the
            kube-apiserver is higher than 1s.
          summary: Kubernetes apiserver latency is high
      - alert: K8SApiServerEtcdAccessLatency
        expr: etcd_request_latencies_summary{quantile="0.99"} / 1e+06 > 1
        for: 15m
        labels:
          service: k8s
          severity: warning
        annotations:
          description: 99th percentile latency for apiserver to access etcd is higher
            than 1s.
          summary: Access to etcd is slow
      - alert: K8SKubeletTooManyPods
        expr: kubelet_running_pod_count > 100
        labels:
          service: k8s
          severity: warning
        annotations:
          description: Kubelet {{$labels.instance}} is running {{$value}} pods, close
            to the limit of 110
          summary: Kubelet is close to pod limit
      - alert: K8SPodWaiting
        expr: sum by(job, exported_pod, instance, namespace) (sum_over_time(kube_pod_container_status_waiting[5m])
          > 0)
        for: 5m
        labels:
          service: k8s
          severity: critical
        annotations:
          description: Pod {{ $labels.namespace }}/{{ $labels.exported_pod }} is in waiting
            state
          summary: Pod is waiting too long
      - alert: K8sPodRestartingTooMuch
        expr: sum by(job, exported_pod, instance, namespace) (rate(kube_pod_container_status_restarts[5m])
          > 0)
        for: 5m
        labels:
          service: k8s
          severity: critical
        annotations:
          description: Pod {{ $labels.namespace }}/{{ $labels.exported_pod }} is restarting
            constantly
          summary: Pod is in a restart loop
